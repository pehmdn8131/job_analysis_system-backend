from DrissionPage import ChromiumPage
from DrissionPage import ChromiumOptions
from app import app, db, Job
import time, random, re, hashlib, json, os
from html import unescape
from datetime import datetime

KEYWORD = "Java"
MAX_PAGES = 2
PROGRESS_FILE = "progress.json"


# ========== è–ªèµ„æ¸…æ´— ==========
def clean_salary(salary_str):
    if not salary_str:
        return 0, 0
    s = salary_str.lower()
    nums = re.findall(r'\d+\.?\d*', s)
    if not nums:
        return 0, 0
    if 'ä¸‡' in s:
        min_sal = float(nums[0]) * 10000
        max_sal = float(nums[1]) * 10000 if len(nums) > 1 else min_sal
        if 'å¹´' in s:
            min_sal /= 12
            max_sal /= 12
    else:
        min_sal = float(nums[0]) * 1000
        max_sal = float(nums[1]) * 1000 if len(nums) > 1 else min_sal
    return int(min_sal), int(max_sal)


# ========== å”¯ä¸€å“ˆå¸Œ ==========
def gen_hash(job_name, company, job_id=None):
    if job_id:
        return hashlib.md5(f"{job_id}_{job_name}_{company}".encode()).hexdigest()
    return hashlib.md5(f"{job_name}_{company}".encode()).hexdigest()


# ========== (æ–°å¢) æå–å¡ç‰‡ä¸Šçš„æŠ€æœ¯æ ‡ç­¾ ==========
def extract_card_tags(card):
    """
    ä»åˆ—è¡¨å¡ç‰‡ä¸­æå–æ ‡ç­¾ï¼Œå¹¶è¿‡æ»¤æ‰ç¦åˆ©å¾…é‡ç±»çš„è¯
    """
    technical_tags = []

    # 1. å®šä¹‰ç¦åˆ©å¾…é‡å…³é”®è¯ï¼ˆé»‘åå•ï¼‰
    # åªè¦æ ‡ç­¾åŒ…å«è¿™äº›è¯ï¼Œå°±è®¤ä¸ºå®ƒä¸æ˜¯æŠ€æœ¯æ ˆ
    welfare_blacklist = [
        "äº”é™©", "ä¸€é‡‘", "ç¤¾ä¿", "å…¬ç§¯é‡‘",
        "åŒä¼‘", "å•åŒ", "ä¼‘", "å‡", "å¹´å‡",
        "è¡¥", "é¤", "æˆ¿", "åŒ…åƒ", "åŒ…ä½",
        "å¥–", "è–ª", "çº¢", "ææˆ",
        "ä½“æ£€", "æ—…æ¸¸", "å›¢å»º", "èŠ‚æ—¥", "ç”Ÿæ—¥",
        "å¼¹æ€§", "æ°›å›´", "é›¶é£Ÿ", "ä¸‹åˆèŒ¶", "æœŸæƒ",
        "æ™‹å‡", "åŸ¹è®­", "æ‰å¹³", "é¢†å¯¼å¥½", "è‚¡ç¥¨",
        "å…è´¹", "äº¤é€š", "é€šè®¯", "é‡‡æš–", "é«˜æ¸©"
    ]

    try:
        # è·å–å¡ç‰‡å†…æ‰€æœ‰çš„ tag å…ƒç´ 
        # æ ¹æ®ä½ çš„æ—¥å¿—ï¼Œclass å¯èƒ½æ˜¯ 'tag'
        tags = card.eles('.tag')

        for tag in tags:
            text = tag.text.strip()
            if not text:
                continue

            # æ£€æŸ¥æ˜¯å¦åŒ…å«é»‘åå•è¯æ±‡
            is_welfare = False
            for bad_word in welfare_blacklist:
                if bad_word in text:
                    is_welfare = True
                    break

            # å¦‚æœä¸æ˜¯ç¦åˆ©è¯ï¼Œä¸”é•¿åº¦é€‚ä¸­ï¼ˆæ’é™¤å¤ªé•¿çš„åºŸè¯ï¼‰ï¼Œè®¤ä¸ºæ˜¯æŠ€æœ¯è¯
            if not is_welfare and len(text) < 15:
                technical_tags.append(text)

    except Exception as e:
        pass  # æå–å¤±è´¥å°±ç®—äº†ï¼Œä¸å½±å“ä¸»æµç¨‹

    # å»é‡å¹¶ç”¨é€—å·æ‹¼æ¥
    return ",".join(list(set(technical_tags)))


# ========== è§£æè¯¦æƒ…é¡µ ==========
def parse_detail_page(tab):
    education = "ä¸é™"
    skills = ""

    try:
        tab.wait.ele_displayed('body', timeout=3)
    except:
        pass

    try:
        if tab.ele('.login_layer_close', timeout=1):
            tab.ele('.login_layer_close').click()
    except:
        pass

    desc = ""
    selectors = ['.bmsg.job_msg.inbox', '.job_msg', '.job-detail']

    for selector in selectors:
        try:
            elem = tab.ele(selector, timeout=0.5)
            if elem:
                desc = elem.text
                break
        except:
            continue

    if not desc:
        try:
            desc = tab.ele('body', timeout=0.5).text[:1000]
        except:
            return education, skills

    # æå–æŠ€èƒ½ (è¿™é‡Œä¿ç•™è¯¦æƒ…é¡µæå–é€»è¾‘ï¼Œä½œä¸ºè¡¥å……)
    skill_keywords = ["Python", "Flask", "Django", "MySQL", "Linux", "Docker", "Git",
                      "Redis", "Vue", "React", "Java", "C++", "ç®—æ³•", "åç«¯", "å…¨æ ˆ"]
    skills_found = []
    desc_upper = desc.upper()
    for keyword in skill_keywords:
        if keyword.upper() in desc_upper:
            skills_found.append(keyword)

    skills = ",".join(list(set(skills_found)))[:100]

    return education, skills


# ========== è¿›åº¦ä¿å­˜ ==========
def save_progress(page_num):
    with open(PROGRESS_FILE, "w") as f:
        json.dump({"page": page_num}, f)


def load_progress():
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE) as f:
            return json.load(f).get("page", 1)
    return 1


def get_company_name(card):
    company = "æœªçŸ¥å…¬å¸"
    try:
        company_links = card.eles('a.cname')
        if company_links:
            return company_links[0].text.strip()

        if company == "æœªçŸ¥å…¬å¸":
            lines = [line.strip() for line in card.text.split('\n') if line.strip()]
            for line in lines:
                if len(line) > 2 and 'å…¬å¸' in line and not any(char.isdigit() for char in line[:5]):
                    company = line
                    break
    except:
        pass
    return company


def get_job_detail_url(job_id, job_name):
    if not job_id: return None
    return f"https://jobs.51job.com/all/{job_id}.html"


# ========== ä¸»ç¨‹åº ==========
def run_spider():
    start_page = 1

    co = ChromiumOptions()
    co.set_argument('--blink-settings=imagesEnabled=false')
    page = ChromiumPage(co)
    page.set.timeouts(5)
    page.set.download_path('.')
    page.set.window.max()

    search_url = f'https://we.51job.com/pc/search?keyword={KEYWORD}'
    print(f"æ­£åœ¨è®¿é—®: {search_url}")
    page.get(search_url)

    try:
        page.wait.ele_displayed('.joblist-item-job-wrapper', timeout=20)
    except:
        page.wait.load_start()

    try:
        for page_num in range(start_page, MAX_PAGES + 1):
            print(f"\n===== ç¬¬ {page_num} é¡µ =====")

            print("æ­£åœ¨æ»šåŠ¨åŠ è½½...")
            for i in range(3):
                page.scroll.to_bottom()
                time.sleep(0.5)

            cards = page.eles('.joblist-item-job-wrapper')
            print(f"æ‰¾åˆ° {len(cards)} ä¸ªèŒä½å¡ç‰‡")

            if not cards:
                print("âŒ æœªæ‰¾åˆ°èŒä½å¡ç‰‡")
                break

            for i, card in enumerate(cards):
                try:
                    # 1. è·å–åŸºç¡€æ•°æ®
                    sensors_div = None
                    try:
                        divs = card.eles('tag:div')
                        for div in divs:
                            if div.attr('sensorsname') == 'JobShortExposure':
                                sensors_div = div
                                break
                    except:
                        pass

                    if not sensors_div: continue

                    sensors_data_str = sensors_div.attr('sensorsdata')
                    if not sensors_data_str: continue

                    data = json.loads(unescape(sensors_data_str))

                    job_id = data.get("jobId", "")
                    job_name = data.get("jobTitle", "").strip()
                    salary_str = data.get("jobSalary", "").strip()
                    city = data.get("jobArea", "").strip()
                    experience = data.get("jobYear", "").strip()
                    education = data.get("jobDegree", "ä¸é™").strip()

                    if not job_name: continue

                    company = get_company_name(card)
                    s_min, s_max = clean_salary(salary_str)
                    detail_url = get_job_detail_url(job_id, job_name)
                    job_hash = gen_hash(job_name, company, job_id)

                    # 2. æ£€æŸ¥æ•°æ®åº“
                    with app.app_context():
                        exists = Job.query.filter_by(hash=job_hash).first()

                    if exists:
                        print(f"â­ï¸ å·²å­˜åœ¨: {job_name}")
                        continue

                    # ===============================================
                    # âœ… æ ¸å¿ƒä¿®æ”¹ï¼šå…ˆä»å¡ç‰‡æå–æŠ€èƒ½æ ‡ç­¾
                    # ===============================================
                    card_skills = extract_card_tags(card)
                    skills = card_skills  # é»˜è®¤ä½¿ç”¨å¡ç‰‡æŠ€èƒ½

                    edu_detail = education

                    # 3. (å¯é€‰) çˆ¬å–è¯¦æƒ…é¡µ
                    # å¦‚æœä½ å¸Œæœ›åªç”¨å¡ç‰‡æ•°æ®ï¼Œå¯ä»¥æŠŠä¸‹é¢è¿™ä¸ª if detail_url å—æ³¨é‡Šæ‰ï¼Œé€Ÿåº¦ä¼šæå¿«ï¼
                    # å¦‚æœä½ æƒ³åˆå¹¶ä¸¤è€…ï¼Œä¿ç•™ä¸‹é¢çš„ä»£ç 
                    if detail_url:
                        try:
                            tab = page.new_tab(detail_url)
                            tab.wait.load_start()
                            if 'login' not in tab.url:
                                edu_extracted, skills_extracted = parse_detail_page(tab)
                                if edu_extracted and edu_extracted != "ä¸é™":
                                    edu_detail = edu_extracted

                                # åˆå¹¶æŠ€èƒ½ï¼šå¡ç‰‡æŠ€èƒ½ + è¯¦æƒ…é¡µæŠ€èƒ½
                                combined_skills = set(card_skills.split(',')) | set(skills_extracted.split(','))
                                # å»é™¤ç©ºå­—ç¬¦ä¸²
                                combined_skills.discard('')
                                skills = ",".join(list(combined_skills))

                            tab.close()
                        except:
                            try:
                                tab.close()
                            except:
                                pass

                        time.sleep(random.uniform(0.5, 1.0))

                    # 4. ä¿å­˜
                    try:
                        with app.app_context():
                            job_obj = Job(
                                job_name=job_name,
                                salary=salary_str,
                                salary_min=s_min,
                                salary_max=s_max,
                                city=city,
                                experience=experience,
                                education=edu_detail,
                                skills=skills,  # è¿™é‡Œå­˜å…¥çš„å°±æ˜¯è¿‡æ»¤åçš„æŠ€æœ¯æ ‡ç­¾
                                company=company,
                                detail_url=detail_url,
                                hash=job_hash,
                                create_time=datetime.now()
                            )
                            db.session.add(job_obj)
                            db.session.commit()
                            print(f"ğŸ’¾ å·²ä¿å­˜: {job_name} | æŠ€èƒ½: {skills[:30]}...")
                    except Exception as e:
                        print(f"æ•°æ®åº“ä¿å­˜å‡ºé”™: {e}")

                except Exception as e:
                    print(f"å¡ç‰‡å¤„ç†å‡ºé”™: {e}")
                    continue

            save_progress(page_num + 1)

            # ç¿»é¡µé€»è¾‘
            if page_num < MAX_PAGES:
                print(f"\nå‡†å¤‡ç¿»é¡µ: ç¬¬ {page_num} -> {page_num + 1} é¡µ...")
                try:
                    old_first_job = cards[0].ele('.job-info').text[:10]
                except:
                    old_first_job = "unknown"

                next_success = False
                try:
                    next_btn = page.ele('css:button.btn-next', timeout=2) or \
                               page.ele('xpath://button[contains(text(), "ä¸‹ä¸€é¡µ")]', timeout=2) or \
                               page.ele('css:li.next', timeout=2)

                    if next_btn and 'disabled' not in (next_btn.attr('class') or ''):
                        next_btn.scroll.to_see()
                        time.sleep(0.5)
                        next_btn.click()
                        print("ğŸ–±ï¸ ç‚¹å‡»äº†ä¸‹ä¸€é¡µæŒ‰é’®")
                        next_success = True
                except:
                    pass

                if not next_success:
                    next_url = f'https://we.51job.com/pc/search?keyword={KEYWORD}&p={page_num + 1}'
                    print(f"ğŸ”— å°è¯•URLè·³è½¬: {next_url}")
                    page.get(next_url)

                print("â³ ç­‰å¾…æ•°æ®æ›´æ–°...")
                is_new_page = False
                check_start = time.time()
                while time.time() - check_start < 10:
                    try:
                        new_cards = page.eles('.joblist-item-job-wrapper')
                        if new_cards:
                            new_first_job = new_cards[0].ele('.job-info').text[:10]
                            if new_first_job != old_first_job:
                                is_new_page = True
                                print(f"âœ… ç¿»é¡µæˆåŠŸ")
                                break
                    except:
                        pass
                    time.sleep(1)

                time.sleep(2)
            else:
                break

    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·æ‰‹åŠ¨åœæ­¢ç¨‹åº")
    finally:
        page.quit()


if __name__ == '__main__':
    run_spider()
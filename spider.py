# spider.py
from DrissionPage import ChromiumPage, ChromiumOptions
from models import db, Job
import time, re, hashlib, json, random
from html import unescape
from datetime import datetime

# ==========================================
# ğŸ”¥ å…¨å±€çŠ¶æ€å˜é‡ (ç”¨äºå‰ç«¯è¿›åº¦æ¡)
# ==========================================
spider_status = {
    'is_running': False,
    'total_added': 0,
    'current_page': 0,
    'log': 'å°±ç»ª'
}


# ========== 1. å·¥å…·å‡½æ•° (ä¿ç•™æ—§ç‰ˆé€»è¾‘) ==========

def clean_salary(salary_str):
    if not salary_str: return 0, 0
    s = salary_str.lower()
    nums = re.findall(r'\d+\.?\d*', s)
    if not nums: return 0, 0
    multiplier = 1
    if 'ä¸‡' in s:
        multiplier = 10000
    elif 'åƒ' in s:
        multiplier = 1000
    is_year = 'å¹´' in s
    try:
        min_sal = float(nums[0]) * multiplier
        max_sal = float(nums[1]) * multiplier if len(nums) > 1 else min_sal
        if is_year:
            min_sal /= 12
            max_sal /= 12
        return int(min_sal), int(max_sal)
    except:
        return 0, 0


def gen_hash(job_name, company, job_id=None):
    if job_id:
        return hashlib.md5(f"{job_id}_{job_name}_{company}".encode()).hexdigest()
    return hashlib.md5(f"{job_name}_{company}".encode()).hexdigest()


def extract_card_tags(card):
    """æå–æ ‡ç­¾ (ä¿ç•™æ–°ç‰ˆçš„é»‘åå•æœºåˆ¶ï¼Œæ•ˆæœæ›´å¥½)"""
    technical_tags = []
    welfare_blacklist = [
        "äº”é™©", "ä¸€é‡‘", "åŒä¼‘", "åŒ…åƒ", "åŒ…ä½", "å¹´å‡", "èŠ‚æ—¥", "æ—…æ¸¸", "è¡¥", "å‡", "å¥–", "æ°›å›´",
        "ä¸‹åˆèŒ¶", "ä½“æ£€", "å¹´ç»ˆ", "è‚¡ç¥¨", "æœŸæƒ", "å¼¹æ€§", "å…è´¹", "ç­è½¦", "æ™‹å‡", "åŸ¹è®­",
        "å®šæœŸ", "å›¢å»º", "é«˜æ¸©", "é‡‡æš–", "é€šè®¯", "äº¤é€š", "é¤è¡¥", "æˆ¿è¡¥", "å‘¨æœ«", "å¸¦è–ª", "ç»©æ•ˆ", "å…¨å‹¤",
        "å¸‚åœº", "å®¢æˆ·", "ç»´æŠ¤", "é”€å”®", "æˆ˜ç•¥", "å¼€æ‹“", "æ‹›å•†", "è¿è¥"
    ]
    try:
        tags = card.eles('.tag', timeout=0.2)
        for tag in tags:
            text = tag.text.strip()
            if not text: continue
            is_welfare = any(bad in text for bad in welfare_blacklist)
            if not is_welfare and len(text) < 15:
                technical_tags.append(text)
    except:
        pass
    return ",".join(list(set(technical_tags)))


def get_company_name(card):
    """
    ğŸ”¥ ä½¿ç”¨ä½ æ—§ä»£ç é‡Œçš„é€»è¾‘ï¼Œå› ä¸ºå®ƒåœ¨ä½ é‚£è¾¹æ˜¯å¥½ç”¨çš„
    """
    company = "æœªçŸ¥å…¬å¸"
    try:
        # å°è¯•æ‰¾é“¾æ¥
        company_links = card.eles('a.cname')
        if company_links:
            return company_links[0].text.strip()

        # å…œåº•ï¼šæ–‡æœ¬åˆ†æ
        if company == "æœªçŸ¥å…¬å¸":
            lines = [line.strip() for line in card.text.split('\n') if line.strip()]
            for line in lines:
                if len(line) > 2 and 'å…¬å¸' in line and not any(char.isdigit() for char in line[:5]):
                    company = line
                    break
    except:
        pass
    return company


# ========== æ ¸å¿ƒä»»åŠ¡ ==========

def run_spider_task(keyword, target_pages=1):
    """
    keyword: æœç´¢å…³é”®è¯
    target_pages: æœŸæœ›è·å–çš„æ–°æ•°æ®é¡µæ•° (æ¯é¡µæŒ‰50æ¡è®¡ç®—)
    """
    global spider_status

    # ==========================================
    # ğŸ¯ è®¾å®šç›®æ ‡
    # 51job æ¯é¡µçº¦ 20 æ¡ã€‚å¦‚æœç”¨æˆ·æƒ³è¦ 3 é¡µæ–°æ•°æ®ï¼Œ
    # ç›®æ ‡å°±æ˜¯æ‰¾åˆ° 3 * 50 = 150 æ¡æ–°å²—ä½ã€‚
    # ==========================================
    TARGET_NEW_JOBS = target_pages * 20

    # ğŸ›¡ï¸ å®‰å…¨ç†”æ–­ï¼šé˜²æ­¢å…¨ç½‘åªæœ‰1é¡µæ•°æ®ï¼Œçˆ¬è™«å´æƒ³æ‰¾10é¡µï¼Œå¯¼è‡´æ­»å¾ªç¯ã€‚
    # é™åˆ¶æœ€å¤šå‘åç¿»å¤šå°‘é¡µï¼ˆæ¯”å¦‚æœ€å¤šç¿» 50 é¡µï¼‰
    MAX_SCAN_DEPTH = 50

    # 1. é‡ç½®çŠ¶æ€
    spider_status['is_running'] = True
    spider_status['total_added'] = 0
    spider_status['current_page'] = 0
    spider_status['log'] = f"æ­£åœ¨å¯åŠ¨æµè§ˆå™¨æœç´¢: {keyword}..."

    print(f"ğŸ•·ï¸ å¯åŠ¨çˆ¬è™«ä»»åŠ¡: {keyword}")
    print(f"ğŸ¯ ç›®æ ‡: è·å– {target_pages} é¡µæ–°æ•°æ® (çº¦ {TARGET_NEW_JOBS} æ¡)")

    co = ChromiumOptions()
    co.set_argument('--blink-settings=imagesEnabled=false')

    page = ChromiumPage(co)
    page.set.timeouts(5)

    new_jobs_count = 0  # å½“å‰æ¬¡è¿è¡Œæ–°å¢çš„æ•°é‡
    current_page_num = 1  # å½“å‰æ­£åœ¨çˆ¬å–çš„é¡µç 

    try:
        search_url = f'https://we.51job.com/pc/search?keyword={keyword}'
        page.get(search_url)

        try:
            page.wait.ele_displayed('.joblist-item-job-wrapper', timeout=10)
        except:
            spider_status['log'] = "é¡µé¢åŠ è½½è¾ƒæ…¢..."

        # ==========================================
        # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨ while å¾ªç¯ç›´åˆ°ç›®æ ‡è¾¾æˆ
        # ==========================================
        while new_jobs_count < TARGET_NEW_JOBS and current_page_num <= MAX_SCAN_DEPTH:

            spider_status['current_page'] = current_page_num
            status_msg = f"æ­£åœ¨å¤„ç†ç¬¬ {current_page_num} é¡µ | è¿›åº¦: {new_jobs_count}/{TARGET_NEW_JOBS}"
            spider_status['log'] = status_msg
            print(f"\nğŸ“„ {status_msg}")

            # æ»šåŠ¨åŠ è½½
            for i in range(3):
                page.scroll.to_bottom()
                time.sleep(0.5)

            cards = page.eles('.joblist-item-job-wrapper')
            if not cards:
                print("âŒ æœªæ‰¾åˆ°èŒä½å¡ç‰‡ (å¯èƒ½æ˜¯ç¿»åˆ°åº•äº†)")
                break

            total_cards = len(cards)

            # --- å¼€å§‹è§£ææœ¬é¡µ ---
            for i, card in enumerate(cards):
                # å¦‚æœå·²ç»è¾¾åˆ°ç›®æ ‡ï¼Œç›´æ¥è·³å‡ºå¡ç‰‡å¾ªç¯
                if new_jobs_count >= TARGET_NEW_JOBS:
                    break

                spider_status[
                    'log'] = f"ç¬¬ {current_page_num} é¡µ: è§£æ {i + 1}/{total_cards} | å·²å…¥åº“: {new_jobs_count}"

                try:
                    # --- å…ƒç´ å®šä½é€»è¾‘ (ä¿æŒä¸å˜) ---
                    sensors_div = None
                    try:
                        divs = card.eles('tag:div')
                        for div in divs:
                            if div.attr('sensorsname') == 'JobShortExposure':
                                sensors_div = div
                                break
                    except:
                        pass

                    if not sensors_div: continue
                    sensors_data_str = sensors_div.attr('sensorsdata')
                    if not sensors_data_str: continue
                    data = json.loads(unescape(sensors_data_str))

                    job_id = data.get("jobId", "")
                    job_name = data.get("jobTitle", "").strip()
                    if not job_name: continue

                    salary_str = data.get("jobSalary", "").strip()
                    city = data.get("jobArea", "").strip()
                    experience = data.get("jobYear", "").strip()
                    education = data.get("jobDegree", "ä¸é™").strip()

                    company = data.get("jobCompanyName", "").strip()
                    if not company:
                        company = get_company_name(card)

                    s_min, s_max = clean_salary(salary_str)
                    detail_url = f"https://jobs.51job.com/all/{job_id}.html"
                    job_hash = gen_hash(job_name, company, job_id)

                    # æŸ¥é‡
                    if Job.query.filter_by(hash=job_hash).first():
                        # print(f"  è·³è¿‡é‡å¤: {job_name}")
                        continue

                    skills = extract_card_tags(card)

                    job_obj = Job(
                        job_name=job_name,
                        company=company,
                        salary=salary_str,
                        salary_min=s_min,
                        salary_max=s_max,
                        city=city,
                        experience=experience,
                        education=education,
                        skills=skills,
                        detail_url=detail_url,
                        hash=job_hash,
                        create_time=datetime.now()
                    )

                    # å®æ—¶å…¥åº“
                    db.session.add(job_obj)
                    db.session.commit()

                    new_jobs_count += 1
                    spider_status['total_added'] = new_jobs_count
                    print(f"  âœ… ({new_jobs_count}/{TARGET_NEW_JOBS}) {job_name}")

                except Exception as e:
                    db.session.rollback()
                    continue

            # --- æœ¬é¡µå¾ªç¯ç»“æŸ ---

            # å¦‚æœå·²ç»è¾¾æˆç›®æ ‡ï¼Œé€€å‡ºæœ€å¤–å±‚ while å¾ªç¯
            if new_jobs_count >= TARGET_NEW_JOBS:
                spider_status['log'] = "ğŸ‰ ç›®æ ‡è¾¾æˆï¼Œåœæ­¢é‡‡é›†"
                print("ğŸ‰ å·²é‡‡é›†åˆ°è¶³å¤Ÿçš„æ–°æ•°æ®ï¼Œä»»åŠ¡ç»“æŸã€‚")
                break

            # ç¿»é¡µé€»è¾‘
            try:
                next_btn = page.ele('css:button.btn-next', timeout=2) or \
                           page.ele('xpath://button[contains(text(), "ä¸‹ä¸€é¡µ")]', timeout=2) or \
                           page.ele('css:li.next', timeout=2)

                if next_btn and 'disabled' not in (next_btn.attr('class') or ''):
                    next_btn.click()
                    time.sleep(2)
                    current_page_num += 1  # é¡µç +1
                else:
                    print("ğŸš« æ²¡æœ‰ä¸‹ä¸€é¡µäº†ï¼Œåœæ­¢é‡‡é›†ã€‚")
                    break
            except:
                print("ğŸš« ç¿»é¡µæŒ‰é’®æœªæ‰¾åˆ°æˆ–å‡ºé”™ï¼Œåœæ­¢ã€‚")
                break

    except Exception as e:
        spider_status['log'] = f"å‡ºé”™: {str(e)}"
        print(f"âŒ çˆ¬è™«å…¨å±€å¼‚å¸¸: {e}")
    finally:
        page.quit()
        spider_status['is_running'] = False
        spider_status['log'] = f"é‡‡é›†å®Œæˆï¼Œå…±å…¥åº“ {new_jobs_count} æ¡"

    return new_jobs_count